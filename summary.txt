┌─────────────────────────┐
│        IMAGE             │
│   "Dog playing ball"     │
└─────────────┬───────────┘
              │
              ▼
  ┌─────────────────────┐
  │  Pretrained CNN     │
  │ (e.g., VGG16/ResNet)│
  └─────────┬───────────┘
            │
            ▼
     Dense Layer 256 (dense1)
            │
            │
            │
            ├───────────────┐
            │               │
            ▼               ▼
   Caption so far:       LSTM Layer 256 (lstm1)
 "A dog plays"           (remembers word sequence)
      │
      ▼
  Tokenize + Embed → [256-dim vector per word]
      │
      ▼
   Dropout → LSTM → lstm1 vector
            │
            │
            ▼
        ADD LAYER
   merger = add([dense1, lstm1])
            │
            ▼
     Dense Layer 256
            │
            ▼
   Dense Layer vocab_size
        (Softmax)
            │
            ▼
  NEXT WORD PREDICTION
      e.g., "ball"
            │
            ▼
 Feed back "ball" → LSTM to predict next word
 Repeat until caption ends


Image Captioning

Imagine we have:

Image: A photo of a dog playing with a ball

Captions:

“A dog plays ball”

“Dog running in park”

“A ball near dog”

“Happy dog chasing ball”

“Dog jumps for ball”

We want the model to look at the image and the words already generated and predict the next word.

2️⃣ Step 1: Extract image features

Feed the image into a pretrained CNN (e.g., VGG16).

It outputs a vector of numbers representing the image content.

Example vector (simplified):

dense1 = [0.5, 0.1, 0.9, 0.0, ...]  # size 256


Think of this as the “essence” of the image in numbers.

It knows there is a dog, a ball, grass, etc.

3️⃣ Step 2: Process caption with LSTM

Take a caption, e.g., "A dog plays".

Convert each word into numbers (token IDs): [1, 2, 3].

Pad to maxCaptionLength = 5: [1, 2, 3, 0, 0]

Embedding layer converts these IDs into vectors:

embed1 = [
 [0.2,0.1,...],   # "A"
 [0.5,0.3,...],   # "dog"
 [0.1,0.7,...],   # "plays"
 [0,0,...],       # padding
 [0,0,...]        # padding
]


LSTM processes these vectors sequentially and outputs a single vector (lstm1) of size 256:

lstm1 = [0.4, 0.6, 0.1, ...]  # remembers "A dog plays"


LSTM remembers context: it knows that after "A dog plays", words like “ball” or “in park” are likely.

4️⃣ Step 3: Merge image + word features
merger = add([dense1, lstm1])


dense1 → 256-dim vector from the image

lstm1 → 256-dim vector from the words

add → element-wise addition:

merger[i] = dense1[i] + lstm1[i]  for each i=1..256


Combines what the image says with what the words say.

Think of it like joining visual hints + memory of words to make a better guess for the next word.

Example (simplified numbers, size=5):

dense1 = [0.5, 0.1, 0.9, 0.0, 0.3]
lstm1  = [0.4, 0.6, 0.1, 0.2, 0.5]
merger = [0.9, 0.7, 1.0, 0.2, 0.8]

5️⃣ Step 4: Predict the next word

merger vector is passed through more Dense layers and finally softmax.

Softmax outputs probabilities for each word in the vocabulary.

The word with the highest probability is selected as the next word.

Example:

Input caption so far: "A dog plays"
Predicted next word: "ball"


New caption: "A dog plays ball" → feed back to LSTM to predict the next word.

Repeat until end of sentence.

6️⃣ Diagram of add([dense1, lstm1])
      IMAGE FEATURES (dense1)
        [0.5, 0.1, 0.9, 0.0, 0.3]
                  │
                  │
                  ▼
           +----------------+
           |     ADD        |  → merger
           +----------------+
                  ▲
                  │
      WORD FEATURES (lstm1)
        [0.4, 0.6, 0.1, 0.2, 0.5]

                  │
                  ▼
           MERGED VECTOR
        [0.9, 0.7, 1.0, 0.2, 0.8]


Each number in dense1 and lstm1 is added element by element.

Resulting vector now contains both image + caption info.

✅ Layman Summary

Image → dense1 vector (visual info)

Caption words → LSTM vector (context info)

add([dense1, lstm1]) → combine both

Combined vector → Dense + Softmax → predict next word

Repeat until full caption is generated.


