# image-caption-generation

This project tackles the problem of automatically generating descriptive captions for images using deep learning. We implemented an encoder-decoder architecture for image captioning, leveraging pretrained VGG16 features as the encoder and a Recurrent Neural Network (RNN) as the decoder.

We experimented with two main model architectures:
1. Inject Model: Injects image features directly into the RNN at each timestep.
2. Merge Model: Processes image and caption separately, then merges the features before generating the output.

We explored different dropout rates and activation functions (RELU vs SELU) to compare model performance and avoid overfitting.

.
├── CaptionGenerator.py       # Generate captions for a new image using a trained model.
├── DataPreprocessing.py      # Prepare image features extraction & caption cleaning
├── ModelTraining.py          # Defines and train the encoder-decoder RNN to generate captions
├── ModelTesting.py           # Evaluates the model using BLEU scores
├── Features.pkl              # Precomputed image features (generated by preprocessing)
├── Descriptions.csv          # Cleaned captions (generated by preprocessing)
├── models/                   # Folder to store trained model files
└── Flicker8k_Text/           # Contains train/test image lists and captions
